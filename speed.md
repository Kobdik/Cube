# Доступ к деталям

Чем больше глубина аналитики, тем выше вероятность выявления скрытых закономерностей, зёрен роста, так и возможных проблем. Операция детализации (drill-dn) относится к столпам аналитики данных, обращает фокус внимания на детали. Так почему все стандартные инструменты аналитики избегают применения глубокой аналитики? 

В лучшем случае, предлагается последовательный спуск к более глубоким слоям, что сопровождается задержками для подгрузки данных детализации. Для презентации это может и не критично, но для аналитика данных, который хотел бы "глянуть глубже" на нижние уровни и вернуться обратно, не затрачивая времени на обновления системой экрана и не теряя фокуса внимания, такой подход, как минимум, утомляет и раздражает.

# Сложность вычислений и скорость расчёта

Стандартная парадигма расчёта сводных таблиц опирается на SQL-запрос GROUP BY CUBE по колонкам измерений исходной таблицы с аналитическими данными. Иерархии создаются на лету из тех же измерений. Например, для получения иерархии календарных периодов используются колонки с годами, кварталами и месяцами, а для получения иерархии элементов затрат с максимальной глубиной в 4 уровня потребуются 4 колонки для описания каждого уровня.

Для примера, рассмотрим представленный в интерактивной демонстрации сводный отчёт по элементам затрат в разрезе календарных периодов (вторая сводная таблица) при условии, что выбраны все элементы затрат, все подразделения и план 2025 вместе  корректировками.

Такой сводный отчёт мог быть реализован в стандартной парадигме SQL-запроса GROUP BY CUBE по 7 колонкам (4 колонки элементов затрат и 3 колонки календарных периодов), образующим деревья двух его измерений. Оценочно, в лучшем случае, сложность вычислений  составила бы O(2^k * n_rows), где k = 7, n_rows - количество отфильтрованных строк аналитических данных.

Kobdik не боится глубоких деревьев, увеличение вложенности ведёт к увеличению сложности пропорционально сумме средних уровней вложенности всех измерений, оценочно, O(k_avg * n_rows), где k_avg = 6, расчёт будет произведен в 20 раз быстрее. Фильтрация 20 тысяч из 50 тысяч строк аналитических данных и подсчёт итогов занимает 4ms на ноутбуке i7-11370H с двумя процессорами по 3.30 GHz.

Если бы стояла задача обработать 1 млн. отфильтрованных данных при той же конфигурации измерений, то времени на расчёт ушло бы около 200ms, а при стандартном подходе - не менее 4s, что является неприемлемым для OLAP-инструмента, и пришлось бы прибегнуть к последовательному спуску, чтобы растянуть во времени доступ к деталям.

На практике, проектная аналитика зачастую имеет глубину в 6-7 уровней в измерении видов затрат, а календарные периоды - до 4 уровней. Те же 1 млн. отфильтрованных данных в Kobdik будут рассчитаны за 400ms. При этом, обеспечат моментальную детализацию, раскрывая по клику интересующие ветки. Конечно, размер матрицы сводного отчёта может оказаться не в пределах 70 X 20 = 1 400, а в пределах 70 X 800 = 56 000 ячеек, что находится в рабочем диапазоне для Kobdik.
